---
title: "Default Prediction in Co-Branded Credit Card"
output: html_notebook
---

## Loading libraries
Let's begin by loading required libraries.

```{r message=FALSE, warning=FALSE}
library(tidyverse) # several utility packages
library(DataExplorer) # automated EDA
library(Amelia) # missing map
library(Hmisc) # missing values imputation
library(corrplot) # correlation plot
library(gridExtra) # Multipanel plots
library(caret) # Classification & Regression Trees
library(e1071) # Machine Learning Models
library(pROC) # ROC Curve
```

## Loading & Understanding Data

Let's load our dataset and store it in a dataframe. When we opened our dataset in excel, we saw that missing values are denoted by different characters. We will use those while loading our dataset.

```{r}
card_data <- read.csv("Training_dataset_Original.csv", 
                      na.strings = c("na", "N/A", "missing"))
```

Let's take a quick look at the dataset.

```{r}
dim(card_data)

head(card_data)

tail(card_data)
```

We can see that the dataset has 80000 observations and 49 variables. We also have some missing values in our dataset. Let's take a look at the structure and summary of the dataset.

```{r}
glimpse(card_data)
```

```{r}
summary(card_data)
```

Following observations can be made:

    + *Application_key* is just a unique number given to each application/client.
    + Most of the variables are numeric, in fact there is only one factor variable **mvar47**.
    + **default_ind** is our dependent variable, which we are interested in predicting.
    + The dependent variable is loaded as *numeric*, it should be a *factor*.
    + The mean of **default_ind** is 0.2462, indicating that the mean default rate is 24.62%.
    + Many variables are heavily skewed and thus may require transformation.
    + There are a lot of missing values in the dataset.

Let's drop the *application_key* and convert dependent variable to factor.

```{r}
card_data$application_key <- NULL
card_data$default_ind <- as.factor(card_data$default_ind)
```

## Missing Values

Let's see how many missing values are there in the dataset.

```{r}
# missing values per column
missing_per_column <- colSums(sapply(card_data, is.na))
sort(missing_per_column[missing_per_column > 0], decreasing = T)
```

Let's visualize the above information.

```{r fig.width=18}
plot_missing(card_data)
```

We can see that some of the variables have more than 40% missing values. This is not good and we need to choose appropriate strategy to handle missing values. Let's see the relative positions of these missing value in another plot.

```{r fig.width=18}
missmap(card_data, col = c("black", "blue"))
```

We can see that the missing values are not missing randomly. We will have to choose our imputation strategy keeping this in mind.

## EDA

Let's begin our EDA by looking at our dependent variable first.

### Dependent Variable - default_ind

Let's see the distribution of our dependent variable first.

```{r}
prop.table(table(card_data$default_ind))
```

Average default rate for our dataset is around 25%. 

### Independent Variables
Now, we shall look at our independent variables one by one. We will look at univariate as well as bivariate plots of the independent variables w.r.t. our target variable. Let's begin by looking at our only factor variable present in the dataset.

#### Categorical Independent Variable - mvar47

According to the data description, this is the type of product that the applicant applied for. It has only two levels("C" - Charge & "L" - Lending). Let's see its frequency distribution first.

```{r}
prop.table(table(card_data$mvar47))
```

Around 65% of the applicants applied for product type "C". Let's see whether default rates are different for these two product types.

```{r}
card_data %>% ggplot(aes(mvar47, fill = default_ind)) + geom_bar(position = "fill")
```

Indeed, default rates are quite higher for product type "C". Let's move on to remaining numeric independent variables now.

#### Numeric Independent Variables

##### Univariate Analysis

Let's see the distribution of each independent variable first.

```{r}
plot_density(card_data, geom_density_args = list(fill = "blue") ,ncol = 3)
```

In the above univariate plots, we can see that many variables have skewness in them and we might want to transform them before we use them in our models.

##### Bivariate Analysis

Now, let's see the bivarite plot of each independent variable w.r.t. our dependent variable.

```{r message=FALSE, warning=FALSE}
plot_boxplot(card_data, by = "default_ind", geom_boxplot_args = list(fill = "blue"))
```

Following inference can be made after looking at the above plots:

    + *mvar1* i.e. credit worthiness score for idividuals who defaulted, is low compared to individual who didn't.
    + *mvar21* has an impact on default. Average utilization on active revolving credit card loans is higher for individuals who defaulted.
    + *mvar22* has an impact on default. Average utilization of line on all active credit lines activated in last 2 years, is higher for people who defaulted.
    + *mvar25 - mvar32* have an impact on default rate. These variables measure the tenure of credit cards. The individuals who defaulted, generally have a lower tenure.
    + *mvar33* has an impact on default rate. This variable measures the duration of stay at current residential address. Generally, people who defaulted have low duration of stay at their present address.
    + *mvar41* has an impact on default rate. This variable measures the utilization of line on active auto loans. People who defaulted have higher utilization.
    + *mvar42* has an impact on default rate. This variable measures the financial stress of the borrower. Higher the financial stress a borrower has, higher is the chances of default.
    + *mvar44* has an impact on defsult rate. This variable measures the ratio of maximum amount due and total amount due on all active credit lines. Higher the ratio, less is the chances of default.
    
We saw that many variables present in the dataset are measuring similar things like - utilization, amount of credit etc. So we expect that those variables might have a high correlation among themselves. Let's check that.

### Correlations

Let's visualize the correlations between independent variables.

```{r fig.height=18, fig.width=18}
cor_matrix <- card_data %>% select_if(is.numeric) %>% cor(use = "pairwise.complete.obs")
corrplot.mixed(cor_matrix, upper = "circle", lower = "number", tl.pos = "lt")
```

We can see that there is some strong correlation between some of the independent variables. Let's take a closer look at the variables which are highly correlated.

```{r}
high_correlations <- findCorrelation(cor_matrix, cutoff = 0.8, verbose = T, names = T)
```

We can see that following variables have high correlations:

    + *mvar22* & *mvar23* - Average utilization on all active credit cards/lines activated in last 2 and 1 years respectively are highly correlated.
    + *mvar32* & *mvar37* - Sum of tenures of active credit cards is highly correlated with Number of credit cards with an active tenure of at least 2 years.
    + *mavr27* & *mvar26* - Tenure of oldest credit card among all active credit cards is highly correlated to Tenure of oldest revolving credit cards.
    + *mvar10* & *mvar7* - Maximum credit available on active credit lines is highly correlated with Total credit available on accepted credit lines.
    + *mvar16 - mvar20* - Number of active credit cards/lines on which full/atleast 75% credit limit is utilized are highly correlated.
    + *mvar46* & *mvar4* - Severity of default on auto loan(s) is highly correlated with Number of auto loans on which borrower has missed 2 payments.
    
We will be dropping variables from these pairs to avoid multicollinearity in the model.

### Data Cleaning

We saw that we have many missing values in our dataset and also variables are heavily skewed. We will address these issues before proceeding with model building. Let's begin.

#### Missing Values Treatment

To begin with, let's impute the missing values of each variable using simple median.

```{r}
imputed_data <- data.frame(lapply(card_data,function(x) {
    if(is.numeric(x)) ifelse(is.na(x),median(x,na.rm=T),x) else x}))
```

We should now check our variables once again, as our imputation might have chnaged the distribution of original variables.

```{r}
# Rechecking distributions
plot_density(imputed_data, geom_density_args = list(fill = "blue") ,ncol = 3)
```

There is no significant change except that our imputation has brought in some peakedness around the median, which is expected.

#### Outlier Treatment

To deal with outliers, we will be applying log transformation to our variables. Before that, let's identify variables where skewness in more than 1.

```{r}
skew <- imputed_data %>% select_if(is.numeric) %>% sapply(skewness)
skewed_vars <- names(skew[skew > 1 | skew < -1])
skewed_vars
```

Now, let's apply log transformation to these variables.

```{r}
transformed_data <- update_columns(imputed_data, skewed_vars, function(x) log(x + 1))
```

Let's again check distributions of our variables to confirm normality.

```{r}
plot_density(transformed_data, geom_density_args = list(fill = "blue") ,ncol = 3)
```

## Model Building

Now, let's begin our modeling process. We will partition our data into training and test set and use 10 fold cross validation to choose optimum parameters for our model. Let's begin.

### Creating Train & Test Set

```{r}
set.seed(1985)
index <- createDataPartition(transformed_data$default_ind, p = 0.7, list = F)
train <-  transformed_data[index, ] 
test <- transformed_data[-index, ]
```

### Training Model

#### Logistic Regression - Taking All Variables

```{r}
logit_fit <- glm(default_ind ~ ., family = binomial(link = "logit"), data = train)

summary(logit_fit)
```

##### Model Interpretation

The significant variables are highlighted with aesterisk symbol in the coofficients table. The AIC for the model is 46764. This will be observed in subsequent stages when we refine the model. The model having least AIC Score would be the most preferred and optimized one.

**Odds Explanatory Power**
Let's find out the power of Odds and Probability of the variables impacting on default.

```{r}
# Odds Ratio
exp(coef(logit_fit))
```

```{r}
# Probability
exp(coef(logit_fit))/(1+exp(coef(logit_fit)))
```

**Interpretation**

##### Classification Table
Since we have confirmed the importance of additional significant variables, let's check performance of our Model using a Classification Table / Confusion Matrix.

**Classification Table on Training Dataset**

```{r}
pred <- predict(logit_fit, newdata = train, type = "response")
y_pred_num <- ifelse(pred > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0,1))
y_act <- train$default_ind
confusionMatrix(y_pred,y_act,positive="1")
```

**Interpretation:**

    + 5801 out of 8855 borrowers correctly who have been defaulted. This translates to 65% of Positive Predictive Value.
    + 39159 out of 47146 borrowers identified correctly who have been not defaulted. This translates to 83% of Negative Predictive Value.
    + Model Accuracy is 80%.
    + Sensitivity & Specificity are 42% and 92% respectively.
    
**Classification Table on Testing Datset**

```{r}
pred <- predict(logit_fit, newdata = test, type= "response")
y_pred_num <- ifelse(pred > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
y_act <- test$default_ind
confusionMatrix(y_pred, y_act, positive = "1") 
```

**Interpretation:**

    + 2501 out of 3800 borrowers correctly who have been defaulted. This translates to 66% of Positive Predictive Value.
    + 16791 out of 20199 borrowers identified correctly who have been not defaulted. This translates to 83% of Negative Predictive Value.
    + Model Accuracy is 80%.
    + Sensitivity & Specificity are 42% and 93% respectively.
    
Thus, our model shows consistent performance on both train as well as test dataset.

##### ROC Plot

Finally, let's draw the Receiver Operating Characteristic (ROC) plot. It is a plot of the True Positive Rate against the False Positive Rate for the different possible cut-points of a diagnostic test. 

```{r}
auc(y_act, y_pred_num)
plot(roc(y_act, y_pred_num))
```

With AUC for the model is 0.67, the ROC curve demonstrated has a scope for improvement. Let's try to fine tune this model.

##### Model Tuning

We saw that the data set has many predictors. we have included all the explanatory variables in our model. However, selecting the one's which really matters for the model becomes really important. Let's use step-wise selection of our predictors to optimize our model.

```{r}
logit_fit2 <- step(glm(default_ind ~ ., family = binomial(link = "logit"), data = train))
```


